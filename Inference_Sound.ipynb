{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from models.deep_isp_model import DenoisingNet\n",
    "from msr_demosaic import MSRDemosaic\n",
    "from audio_dataset import AudioDataset, AudioGenDataset\n",
    "import deep_isp_utils as utils\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from loss import *\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import quantize\n",
    "import actquant\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "transformation = utils.JointCompose([\n",
    "    utils.JointHorizontalFlip(),\n",
    "    utils.JointVerticalFlip(),\n",
    "    #utils.JointNormailze(means = [0.485,0.456,0.406],stds = [1,1,1]), #TODO consider use\n",
    "    utils.JointToTensor(),\n",
    "])\n",
    "val_transformation = utils.JointCompose([\n",
    "    #utils.JointNormailze(means = [0.485,0.456,0.406],stds = [1,1,1]),\n",
    "    utils.JointToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we put the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_denoise_layers = 20\n",
    "quant = False\n",
    "inject_noise = False\n",
    "quant_bitwidth = 32\n",
    "quant_epoch_step = 50\n",
    "inject_act_noise = False\n",
    "act_bitwidth = 32\n",
    "act_quant = False\n",
    "quant_start_stage = 0\n",
    "weight_relu = False\n",
    "weight_grad_after_quant = False\n",
    "random_inject_noise = False\n",
    "step = 19\n",
    "num_workers = 1\n",
    "wrpn = False\n",
    "\n",
    "gpus = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```load model``` loads a .tar file from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model,checkpoint):\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in checkpoint['state_dict'].items():\n",
    "        name = k[7:] if k[0:6] == 'module.' else k # remove `module. if needed (happen when the model created with DataParallel\n",
    "        #new_state_dict[name] = v\n",
    "        new_state_dict[name] = v if v.dim() > 1 or 'num_batches_tracked' in name else v*v.new_ones(1)\n",
    "\n",
    "    # load params\n",
    "    model.load_state_dict(new_state_dict, strict=False) #strict false in case the loaded doesn't have alll variables like running mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/denoise/Denoising-drone-rotors/models/deep_isp_model.py:53: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(m.weight, mode='fan_out')\n",
      "/home/simon/denoise/Denoising-drone-rotors/models/deep_isp_model.py:55: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint /home/simon/denoise/Denoising-drone-rotors/output/2019-05-17_11-31-44/checkpoint.pth.tar\n"
     ]
    }
   ],
   "source": [
    "model = DenoisingNet(in_channels=1, num_denoise_layers=num_denoise_layers, quant=quant , noise=inject_noise, bitwidth=quant_bitwidth, quant_epoch_step=quant_epoch_step,\n",
    "                         act_noise=inject_act_noise , act_bitwidth= act_bitwidth , act_quant=act_quant, use_cuda=(gpus is not None), quant_start_stage=quant_start_stage,\n",
    "                         weight_relu=weight_relu, weight_grad_after_quant=weight_grad_after_quant, random_inject_noise = random_inject_noise\n",
    "                         , step=step, wrpn=wrpn)\n",
    "model.cuda()\n",
    "device = 'cuda:' + str(0)\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "checkpoint_file = \"/home/simon/denoise/Denoising-drone-rotors/output/2019-05-17_11-31-44/checkpoint.pth.tar\" # checkpoint location\n",
    "if os.path.isfile(checkpoint_file):\n",
    "    print(\"loading checkpoint {}\".format(checkpoint_file))\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    load_model(model, checkpoint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "# datapath = '/home/simon/denoise/dataset/data/data.h5'\n",
    "# testset = MSRDemosaic(root=datapath, train=False, transform=val_transformation)\n",
    "# test_loader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# use train set to check overfitting\n",
    "# testset = AudioDataset(data_h5_path=datapath, train=True)\n",
    "# test_loader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "trainset = AudioGenDataset(\"/home/simon/denoise/dataset/generator/\")\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# testset = AudioDataset(data_h5_path=datapath, train=False)\n",
    "# test_loader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from preprocess_audio.preprocess_audio import *\n",
    "# convert back to audio\n",
    "from preprocess_audio.postprocess_audio import *\n",
    "\n",
    "test_loader = train_loader\n",
    "\n",
    "# ipd.Audio('/home/simon/denoise/dataset/audio/file_example_WAV_1MG.wav')\n",
    "# ipd.Audio(x, rate=sr) # load a NumPy array\n",
    "\n",
    "def unbias_image(img):\n",
    "    return  torch.clamp(img, 0 , 1.).data.squeeze(0).cpu().numpy()#.transpose(1, 2, 0) + 0.5  #the clamp is becuase the value should be between 0-1\n",
    "\n",
    "\n",
    "# Run inference\n",
    "loader = test_loader\n",
    "N_CHANNELS = 513\n",
    "N_FFT = 1024\n",
    "fs = 22050\n",
    "n = 5\n",
    "\n",
    "# create_spectogram\n",
    "# def read_audio_spectum(filename):\n",
    "#     x, fs = librosa.load(filename)\n",
    "#     S = librosa.stft(x, N_FFT)\n",
    "#     p = np.angle(S)\n",
    "#     return np.log1p(np.abs(S[np.newaxis,:,:1000])), fs\n",
    "\n",
    "# a_content, fs = read_audio_spectum(\"/home/simon/denoise/dataset/audio/file_example_WAV_1MG.wav\")\n",
    "# N_SAMPLES = a_content.shape[2]\n",
    "# N_CHANNELS = a_content.shape[1]\n",
    "# # a_style = a_style[:, :N_CHANNELS, :N_SAMPLES]\n",
    "\n",
    "# t = np.zeros_like(a_content)\n",
    "# a = np.zeros_like(a_content[0])\n",
    "# a[:N_CHANNELS,:] = np.exp(t[0]) - 1\n",
    "\n",
    "# # This code is supposed to do phase reconstruction\n",
    "# p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
    "# for i in range(500):\n",
    "#     S = a * np.exp(1j*p)\n",
    "#     x = librosa.istft(S)\n",
    "#     p = np.angle(librosa.stft(x, N_FFT))\n",
    "\n",
    "# librosa.output.write_wav(\"/home/simon/denoise/dataset/audio/file_example_WAV_1MG_re.wav\", x, fs)\n",
    "# ipd.Audio(x, rate=fs) # load a NumPy array\n",
    "def to_image(data):\n",
    "    return data.data.cpu().squeeze(0).squeeze(0).numpy()\n",
    "\n",
    "for batch_idx, (data, target, fname) in enumerate(tqdm(loader)):\n",
    "    \n",
    "    # display noisy sample\n",
    "    plt.figure()\n",
    "    print(\"data source\")\n",
    "    data_image = to_image(data)\n",
    "    print(\"data shape:\", data_image.shape)\n",
    "    print(data_image.max())\n",
    "    plt.imshow(data_image, interpolation='nearest')\n",
    "    plt.show()\n",
    "    y = spectogram_to_wav(data_image, N_CHANNELS, N_FFT, fs, dst_path='/home/simon/denoise/dataset/data/orig_{}.wav'.format(batch_idx))\n",
    "    ipd.display(ipd.Audio(y, rate=fs)) # load a NumPy array\n",
    "    \n",
    "    # display target sample\n",
    "    print(\"target\")\n",
    "    target_image = to_image(target)\n",
    "    print(\"max: \", target_image.max())\n",
    "    print(\"min: \", target_image.min())\n",
    "    plt.imshow(target_image, interpolation='nearest')\n",
    "    plt.show()\n",
    "    t = spectogram_to_wav(target_image, N_CHANNELS, N_FFT, fs, dst_path='/home/simon/denoise/dataset/data/target_{}.wav'.format(batch_idx))\n",
    "    ipd.display(ipd.Audio(t, rate=fs)) # load a NumPy array\n",
    "\n",
    "    # infer noisy sample\n",
    "    if gpus is not None:\n",
    "        data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "    print(data.shape)\n",
    "    output = model(data)\n",
    "#     output *= 1.0 / output.max()\n",
    "#     output = torch.sigmoid(output)\n",
    "#     np_output = unbias_image(output).squeeze(0)\n",
    "    np_output = to_image(output)\n",
    "    # display infered sample\n",
    "    print(\"output\")\n",
    "    print(np_output.max())\n",
    "    print(np_output.min())\n",
    "    plt.imshow(np_output, interpolation='nearest')\n",
    "    plt.show()\n",
    "    dst_file_path = '/home/simon/denoise/dataset/data/test_{}.wav'.format(batch_idx)\n",
    "    x = spectogram_to_wav(np_output, N_CHANNELS, N_FFT, fs, dst_path=None)\n",
    "    ipd.display(ipd.Audio(x, rate=fs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipd.Audio(x, rate=fs) # load a NumPy array\n",
    "ipd.Audio(y, rate=fs) # load a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_sample_file_path = '/home/simon/denoise/dataset/audio/file_example_WAV_1MG.wav'\n",
    "ipd.Audio(audio_sample_file_path) # load a local WAV file\n",
    "\n",
    "# convert to spectogram\n",
    "N_FFT = 1024\n",
    "sound_audio, fs = librosa.load(audio_sample_file_path)\n",
    "spectogram_sound_label, N_CHANNELS = create_spectogram(sound_audio, N_FFT)\n",
    "print(\"from create_spectogram shape: \", spectogram_sound_label.shape)\n",
    "\n",
    "# convert back to audio\n",
    "from preprocess_audio.postprocess_audio import *\n",
    "\n",
    "\n",
    "# hf = h5py.File('/home/simon/denoise/dataset/data/data.h5', 'r')\n",
    "# train = hf.get('train')\n",
    "# input = train.get('input')\n",
    "# train_input_list = list(input.keys())\n",
    "# # print(train_input_list)\n",
    "# specto = np.array(input.get(train_input_list[0]))\n",
    "# print(\"specto shape: \", specto.shape)\n",
    "# x = spectogram_to_wav(specto, 513, 1024, 22050)\n",
    "# print(x.shape)\n",
    "# hf.close()\n",
    "\n",
    "x = spectogram_to_wav(spectogram_sound_label, N_CHANNELS, N_FFT, fs)\n",
    "ipd.Audio(x, rate=fs) # load a local WAV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
